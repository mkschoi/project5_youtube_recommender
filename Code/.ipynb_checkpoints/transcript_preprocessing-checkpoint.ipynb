{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import truecase\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from corenlp_pywrap import pywrap\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v4 = pickle.load(open('../Data/df_videos_cleaned_v4.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the functions (Need to put these into a separate .py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline_1(df):\n",
    "    '''\n",
    "    Input: Dataframe with the raw transcript text in a string format\n",
    "    Output: Dataframe with line breaks, punctuations, and numbers removed from the transcript text\n",
    "    '''\n",
    "    ## Create a function for applying the text preprocessing pipeline\n",
    "    def initial_preprocessing(raw_text):\n",
    "        preprocessed_text_1 = raw_text.replace('\\n', ' ')\n",
    "        preprocessed_text_2 = preprocessed_text_1.translate(str.maketrans('', '', string.punctuation.replace(\"'\", \"\")))\n",
    "        preprocessed_text_3 = re.sub('\\w*\\d\\w*', '', preprocessed_text_2)\n",
    "#         preprocessed_text_4 = truecase.get_true_case(preprocessed_text_3)\n",
    "        \n",
    "        return preprocessed_text_3\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(initial_preprocessing)\n",
    "            \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline_2(df):\n",
    "    '''\n",
    "    Input: Dataframe after applying text_processing_pipeline_1\n",
    "    Output: Dataframe with the transcript further preprocessed - tokenization, stopwords removal, lemmatization\n",
    "    '''\n",
    "    ## Create a function for applying the text preprocessing pipeline\n",
    "    def second_preprocessing(preprocessed_text_3):\n",
    "        stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        \n",
    "        preprocessed_text_4 = nlp(preprocessed_text_3)\n",
    "        preprocessed_text_5 = [word.text for word in preprocessed_text_4 \n",
    "                                    if str(word).lower() not in stopwords and word.text!= ' ']\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "        tag_map['V'] = wordnet.VERB \n",
    "        tag_map['J'] = wordnet.ADJ\n",
    "        tag_map['R'] = wordnet.ADV\n",
    "\n",
    "        preprocessed_text_6 = [lemmatizer.lemmatize(word.lower(), tag_map[tag[0]]) \n",
    "                                    for word, tag in pos_tag(preprocessed_text_5)]\n",
    "        \n",
    "        return preprocessed_text_6\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(second_preprocessing)\n",
    "            \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline_3(df):\n",
    "    '''\n",
    "    Input: Dataframe after applying text_preprocessing_pipeline_2\n",
    "    Output: Dataframe with the transcript further preprocessed - truecasing, part of speech tagging\n",
    "    '''\n",
    "    ## Create a function for applying the text preprocessing pipeline\n",
    "    def third_preprocessing(preprocessed_text_6):\n",
    "        preprocessed_text_7 = ' '.join([word for word in preprocessed_text_6 if len(word)>1])\n",
    "        preprocessed_text_8 = truecase.get_true_case(preprocessed_text_7)\n",
    "        preprocessed_text_9 = nlp(preprocessed_text_8)\n",
    "        preprocessed_text_10 = [(word.text, word.pos_) for word in preprocessed_text_9]\n",
    "        \n",
    "        return preprocessed_text_10\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(third_preprocessing)\n",
    "            \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_df(df_str):\n",
    "    '''\n",
    "    Input: Name of a dataframe in a string format\n",
    "    Output: Pickle the dataframe into the Data folder\n",
    "    '''  \n",
    "    with open('../Data/'+ df_str +'.pickle', 'wb') as f_video_data:\n",
    "        pickle.dump(eval(df_str), f_video_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove line breaks, punctuations, and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v5 = text_preprocessing_pipeline_1(df_videos_cleaned_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_videos_cleaned_v5['Transcript'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, stopwords removal and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v6 = text_preprocessing_pipeline_2(df_videos_cleaned_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(df_videos_cleaned_v6['Transcript'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truecasing, POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v7 = text_preprocessing_pipeline_3(df_videos_cleaned_v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_videos_cleaned_v7['Transcript'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the cleaned dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_df('df_videos_cleaned_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
