{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import os, time, datetime\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the functions (will need to put these into a separate .py file and remove the cells containing functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_link(search_word, num_scrolls):\n",
    "    '''\n",
    "    Input: search query, number of scrolls \n",
    "    Output: links for the video results\n",
    "    '''\n",
    "    \n",
    "    chromedriver = \"/Applications/chromedriver\" ## path to the chromedriver executable\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    \n",
    "    query = search_word\n",
    "    youtube_search = \"https://www.youtube.com/results?search_query=\"\n",
    "    youtube_query = youtube_search + query.replace(' ', '+')\n",
    "    \n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(youtube_query)\n",
    "    \n",
    "    for i in range(num_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "    \n",
    "    user_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')\n",
    "    links = [link for link in [i.get_attribute('href') for i in user_data] if link]\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return links   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_page_scraper(list_links):\n",
    "    '''\n",
    "    Input: a list of links \n",
    "    Output: video data scraped into a dataframe, each row corresponding to a video\n",
    "    '''\n",
    "    chromedriver = \"/Applications/chromedriver\" ## path to the chromedriver executable\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    \n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Video_ID', 'Title', 'Upload Date', 'Duration', 'Views', 'Number of Likes', 'Description', 'Transcript'])\n",
    "    \n",
    "    for link in list_links:\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        ## Video ID\n",
    "        v_id = wait.until(lambda browser: browser.find_elements_by_xpath(\"//ytd-watch-flexy[@class='style-scope ytd-page-manager hide-skeleton']\")[0].get_attribute('video-id'))\n",
    "        \n",
    "        ## Video Title\n",
    "        v_title = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\"))).text\n",
    "        \n",
    "        ## Date\n",
    "        v_date = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#date yt-formatted-string\"))).text\n",
    "        \n",
    "        ## Duration\n",
    "        v_duration = driver.find_elements_by_xpath(\"//span[@class='ytp-time-duration']\")[0].text\n",
    "#         min_sec = time.strptime(duration, '%M:%S')\n",
    "#         v_duration = datetime.timedelta(minutes=min_sec.tm_min, seconds=min_sec.tm_sec).total_seconds()\n",
    "        \n",
    "        ## Views\n",
    "        v_views =  wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#count yt-view-count-renderer\"))).text\n",
    "        \n",
    "        ## Number of likes\n",
    "        v_likes =  wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#top-level-buttons yt-formatted-string\"))).text\n",
    "        \n",
    "        ## Video Description\n",
    "        v_description =  wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div#description yt-formatted-string\"))).text\n",
    "        \n",
    "        ## Transcripts\n",
    "        try:\n",
    "            v_transcript = YouTubeTranscriptApi.get_transcript(v_id)\n",
    "        except:\n",
    "            v_transcript = np.NaN\n",
    "        \n",
    "        df.loc[len(df)] = [v_id, v_title, v_date, v_duration, v_views, v_likes, v_description, v_transcript]\n",
    "\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#     contents_div = soup.find('div', id='contents')\n",
    "#     num_videos = len(contents_div.find_all('a', id='video-title'))\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_video_dataframes(df_videos_raw_str):\n",
    "    df_merged = eval(df_videos_raw_str + '_1')\n",
    "    for i in range(2,16):\n",
    "        df_merged = pd.concat([df_merged, eval(df_videos_raw_str + '_' + str(i))])\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_csv(df_str):\n",
    "    eval(df_str).to_csv('../Data/{}.csv'.format(df_str), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all the relevant educational topics in stock investing as search queries (make sure these are not closely related to prevent search results that are too similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_search_queries = ['fundamental investing', 'value investing', 'growth investing', 'long term investing', \n",
    "                          'stock valuation', 'competitive moats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all the video links of each search result into a list.  Most search results end after 35+ scrolls so will keep our scrolls to 30.  Comment them out afterward to prevent reproducing different sets of video links (this is necessary as YouTube is dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fundamental_investing_links = get_video_link(list_of_search_queries[0], 30)\n",
    "# value_investing_links = get_video_link(list_of_search_queries[1], 30)\n",
    "# growth_investing_links = get_video_link(list_of_search_queries[2], 30)\n",
    "# long_term_investing_links = get_video_link(list_of_search_queries[3], 30)\n",
    "# stock_valuation_links = get_video_link(list_of_search_queries[4], 30)\n",
    "competitive_moats_links = get_video_link(list_of_search_queries[5], 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function that scrapes data from each video page.  Save the resulting dataframe into a variable.  Had to cut the list of video links into smaller chunks due to intermittent buffering (need to figure out a fix).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_raw6_1 = video_page_scraper(competitive_moats_links[0:20])\n",
    "df_videos_raw6_2 = video_page_scraper(competitive_moats_links[20:40])\n",
    "df_videos_raw6_3 = video_page_scraper(competitive_moats_links[40:60])\n",
    "df_videos_raw6_4 = video_page_scraper(competitive_moats_links[60:80])\n",
    "df_videos_raw6_5 = video_page_scraper(competitive_moats_links[80:100])\n",
    "df_videos_raw6_6 = video_page_scraper(competitive_moats_links[100:120])\n",
    "df_videos_raw6_7 = video_page_scraper(competitive_moats_links[120:140])\n",
    "df_videos_raw6_8 = video_page_scraper(competitive_moats_links[140:160])\n",
    "df_videos_raw6_9 = video_page_scraper(competitive_moats_links[160:180])\n",
    "df_videos_raw6_10 = video_page_scraper(competitive_moats_links[180:200])\n",
    "df_videos_raw6_11 = video_page_scraper(competitive_moats_links[200:220])\n",
    "df_videos_raw6_12 = video_page_scraper(competitive_moats_links[220:240])\n",
    "df_videos_raw6_13 = video_page_scraper(competitive_moats_links[240:260])\n",
    "df_videos_raw6_14 = video_page_scraper(competitive_moats_links[260:280])\n",
    "df_videos_raw6_15 = video_page_scraper(competitive_moats_links[280:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the smaller dataframes to create a single dataframe per search query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_videos_raw_fundamental_investing = merge_video_dataframes('df_videos_raw1')\n",
    "# df_videos_raw_value_investing = merge_video_dataframes('df_videos_raw2')\n",
    "# df_videos_raw_growth_investing = merge_video_dataframes('df_videos_raw3')\n",
    "# df_videos_raw_long_term_investing = merge_video_dataframes('df_videos_raw4')\n",
    "# df_videos_raw_stock_valuation = merge_video_dataframes('df_videos_raw5')\n",
    "df_videos_raw_competitive_moats = merge_video_dataframes('df_videos_raw6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataframes into .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_csv('df_videos_raw_fundamental_investing')\n",
    "# df_to_csv('df_videos_raw_value_investing')\n",
    "# df_to_csv('df_videos_raw_growth_investing')\n",
    "# df_to_csv('df_videos_raw_long_term_investing')\n",
    "# df_to_csv('df_videos_raw_stock_valuation')\n",
    "df_to_csv('df_videos_raw_competitive_moats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
