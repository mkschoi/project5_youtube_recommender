{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v7 = pickle.load(open('../Data/df_videos_cleaned_v7.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the functions (Need to put these into a separate .py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_custom_stopwords(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe\n",
    "    Output: Dataframe with custom stopwords removed\n",
    "    '''\n",
    "    def final_preprocessing(cleaned_text):  \n",
    "        nlp.Defaults.stop_words |= {'uh','yeah','man','um','oh','guy','maybe','bye'}\n",
    "        stopwords = nlp.Defaults.stop_words\n",
    "        \n",
    "        preprocessed_text_12 = [(word.lower(), pos) for word, pos in cleaned_text \n",
    "                                    if word.lower() not in stopwords] \n",
    "        \n",
    "        return preprocessed_text_12\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(final_preprocessing)\n",
    "            \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(df, vectorizer):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords) and type of vectorizer\n",
    "    Output: Document-term matrix\n",
    "    '''\n",
    "    ## Take the words out of the (word, POS) tuple, vectorize, and fit-transform into a matrix\n",
    "    word_list = [[word[0] for word in doc] for doc in df['Transcript']]\n",
    "    vec = vectorizer(tokenizer=lambda doc:doc, lowercase=False, min_df=2, max_df=0.3)\n",
    "    matrix = vec.fit_transform(word_list).toarray()\n",
    "        \n",
    "    return matrix, vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(matrix, model, num_topics, num_words):\n",
    "    '''\n",
    "    Input: Document-term matrix, type of topic model, number of topics, and number of words is each topic\n",
    "    Output: a list of lists containing topic words\n",
    "    '''\n",
    "    ## Creates an instance of an NMF or LDA model\n",
    "    if model == NMF:\n",
    "        model = model(num_topics)\n",
    "    elif model == LatentDirichletAllocation:\n",
    "        model = model(n_components=num_topics)\n",
    "        \n",
    "    ## Fit_transform (matrix factorization for NMF) the doc_word matrix to get doc_topic and topic_word matrices\n",
    "    doc_topic = model.fit_transform(matrix)\n",
    "    topic_word = model.components_\n",
    "    \n",
    "    ## Retrieves the top words in each topic\n",
    "    words = document_term_matrix(df_videos_cleaned_v8, CountVectorizer)[1]\n",
    "    t_model = topic_word.argsort(axis=1)[:, -1:-(num_words+1):-1]\n",
    "    top_topic_words = [[words[i] for i in topic] for topic in t_model]\n",
    "        \n",
    "    return top_topic_words, doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_of_adjectives(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords) \n",
    "    Output: Dataframe with only adjectives in the transcript corpus\n",
    "    '''\n",
    "    def adjectives(cleaned_text):\n",
    "        \n",
    "        preprocessed_text_adj = [(word.lower(), pos) for word, pos in cleaned_text \n",
    "                                    if pos=='ADJ'] \n",
    "        \n",
    "        return preprocessed_text_adj\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(adjectives)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_assignment(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords)\n",
    "    Output: Dataframe with topic and topic coefficient added\n",
    "    '''\n",
    "    ## Takes the highest coefficient for each video (row) in the doc_topic matrix, and puts them into a list \n",
    "    doc_topic = topic_model(X_tfidf, NMF, 6, 7)[1]\n",
    "    topic_coeff = [round(np.max(coeffs),3) for coeffs in doc_topic]\n",
    "    topic = list(doc_topic.argmax(axis=1))\n",
    "    \n",
    "    ## Map topic indices to topic names\n",
    "    topic_keys = {0:'General', 1:'Valuation', 2:'Competitive Moats', 3:'Passive Investing', \n",
    "                  4:'Valuation', 5:'Technology Stocks'}\n",
    "    \n",
    "    topic_name = [topic_keys.get(topic_index,'') for topic_index in topic]\n",
    "    \n",
    "    ## Add the Topic and Topic Coefficient columns\n",
    "    df['Topic'] = topic_name\n",
    "    df['Topic Coefficient'] = topic_coeff\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_df(df_str):\n",
    "    '''\n",
    "    Input: Name of a dataframe in a string format\n",
    "    Output: Pickle the dataframe into the Data folder\n",
    "    '''  \n",
    "    with open('../Data/'+ df_str +'.pickle', 'wb') as f_video_data:\n",
    "        pickle.dump(eval(df_str), f_video_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v8 = remove_custom_stopwords(df_videos_cleaned_v7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating document-term matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = document_term_matrix(df_videos_cleaned_v8, CountVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = document_term_matrix(df_videos_cleaned_v8, TfidfVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling - Entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sort', 'buffett', 'cheap', 'moat', 'multiple', 'team', 'life'],\n",
       " ['option', 'leap', 'cover', 'decay', 'spread', 'view', 'ge'],\n",
       " ['equal', 'divide', 'discount', 'constant', 'present', 'zero', 'minus'],\n",
       " ['graham', 'buffett', 'security', 'ben', 'street', 'warren', 'intrinsic'],\n",
       " ['etf', 'holding', 'index', 'tax', 'sector', 'expense', 'goal'],\n",
       " ['bank', 'report', 'news', 'chart', 'support', 'data', 'economy']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nmf_cv = topic_model(X_cv, NMF, 6, 7)[0]\n",
    "topics_nmf_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['buffett', 'sort', 'warren', 'technical', 'bank', 'trader', 'index'],\n",
       " ['constant',\n",
       "  'formula',\n",
       "  'divide',\n",
       "  'discount',\n",
       "  'present',\n",
       "  'equal',\n",
       "  'calculate'],\n",
       " ['moat', 'mode', 'competitive', 'economic', 'brand', 'competitor', 'castle'],\n",
       " ['etf', 'holding', 'index', 'vanguard', 'expense', 'johnson', 'etfs'],\n",
       " ['ebitda', 'enterprise', 'multiple', 'irr', 'forecast', 'statement', 'da'],\n",
       " ['tesla', 'apple', 'pe', 'amazon', 'facebook', 'car', 'vehicle']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nmf_tfidf = topic_model(X_tfidf, NMF, 6, 7)[0]\n",
    "topics_nmf_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['moat', 'customer', 'tesla', 'brand', 'apple', 'competitive', 'mode'],\n",
       " ['bank', 'economy', 'news', 'report', 'country', 'inflation', 'index'],\n",
       " ['equal',\n",
       "  'divide',\n",
       "  'discount',\n",
       "  'formula',\n",
       "  'present',\n",
       "  'calculate',\n",
       "  'constant'],\n",
       " ['buffett', 'sort', 'warren', 'graham', 'cheap', 'berkshire', 'write'],\n",
       " ['pe', 'statement', 'multiple', 'bank', 'sheet', 'ebitda', 'negative'],\n",
       " ['etf', 'index', 'chart', 'tax', 'sector', 'holding', 'goal']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_lda_cv = topic_model(X_cv, LatentDirichletAllocation, 6, 7)[0]\n",
    "topics_lda_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling - Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v8_adj = df_videos_cleaned_v8.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_adj = corpus_of_adjectives(df_videos_cleaned_v8_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_adj = document_term_matrix(df_videos_cleaned_adj, CountVectorizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_adj = document_term_matrix(df_videos_cleaned_adj, TfidfVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['airtel', 'boundaries', 'badge', 'allocation', 'brokendown', 'conversely'],\n",
       " ['broken', 'buoy', 'branding', 'airbus', 'cancel', 'california'],\n",
       " ['arrive', 'alumnus', 'camry', 'allergy', 'breakfast', 'atr'],\n",
       " ['berserk', 'boundaries', 'camry', 'branding', 'alum', 'association'],\n",
       " ['coffin', 'arc', 'branding', 'brainwash', 'brownfield', 'calendar'],\n",
       " ['bowman', 'beck', 'bjp', 'brokendown', 'acknowledge', 'blackberry']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nmf_cv_adj = topic_model(X_cv_adj, NMF, 6, 6)[0]\n",
    "topics_nmf_cv_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beck', 'brokendown', 'airtel', 'broken', 'bowman', 'burdensome'],\n",
       " ['alumnus', 'arrive', 'camry', 'broken', 'allergy', 'cambria'],\n",
       " ['berserk', 'association', 'commensurate', 'camry', 'alum', 'completion'],\n",
       " ['allocation', 'arc', 'coastal', 'contractor', 'beneficial', 'aqr'],\n",
       " ['coffin', 'adolescent', 'ag', 'buoy', 'broken', 'brownfield'],\n",
       " ['branding',\n",
       "  'brainwash',\n",
       "  'boundaries',\n",
       "  'budweiser',\n",
       "  'calendar',\n",
       "  'acknowledge']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nmf_tfidf_adj = topic_model(X_tfidf_adj, NMF, 6, 6)[0]\n",
    "topics_nmf_tfidf_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['broken', 'coffin', 'buoy', 'california', 'airbus', 'ag'],\n",
       " ['airtel', 'allocation', 'arc', 'boundaries', 'asleep', 'badge'],\n",
       " ['awfully', 'airtel', 'badge', 'captive', 'canina', 'allocation'],\n",
       " ['branding', 'brainwash', 'berserk', 'calendar', 'association', 'budweiser'],\n",
       " ['beck', 'brokendown', 'bowman', 'brownfield', 'blackberry', 'buffer'],\n",
       " ['arrive', 'camry', 'alumnus', 'allergy', 'berserk', 'broken']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_lda_cv_adj = topic_model(X_cv_adj, LatentDirichletAllocation, 6, 6)[0]\n",
    "topics_lda_cv_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning topics and coefficients to videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v9 = topic_assignment(df_videos_cleaned_v8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the modified dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_df('df_videos_cleaned_v9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
