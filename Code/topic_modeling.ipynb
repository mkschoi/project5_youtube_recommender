{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v6 = pd.read_csv('../Data/df_videos_cleaned_v6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the functions (Need to put these into a separate .py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_custom_stopwords(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe\n",
    "    Output: Dataframe with custom stopwords removed\n",
    "    '''\n",
    "    def final_preprocessing(cleaned_text):\n",
    "        preprocessed_text_11 = eval(cleaned_text)\n",
    "       \n",
    "        nlp.Defaults.stop_words |= {'uh','yeah','man','um','oh','guy','maybe','bye'}\n",
    "        stopwords = nlp.Defaults.stop_words\n",
    "        \n",
    "        preprocessed_text_12 = [(word.lower(), pos) for word, pos in preprocessed_text_11 \n",
    "                                    if word.lower() not in stopwords] \n",
    "        \n",
    "        return preprocessed_text_12\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(final_preprocessing)\n",
    "            \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(df, vectorizer):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords) and type of vectorizer\n",
    "    Output: Document-term matrix\n",
    "    '''\n",
    "    # Take the words out of the (word, POS) tuple, vectorize, and fit-transform into a matrix\n",
    "    word_list = [[word[0] for word in doc] for doc in df['Transcript']]\n",
    "    vec = vectorizer(tokenizer=lambda doc:doc, lowercase=False, min_df=2, max_df=0.5)\n",
    "    matrix = vec.fit_transform(word_list).toarray()\n",
    "        \n",
    "    return matrix, vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(matrix, model, num_topics, num_words):\n",
    "    '''\n",
    "    Input: Document-term matrix, type of topic model, number of topics, and number of words is each topic\n",
    "    Output: a list of lists containing topic words\n",
    "    '''\n",
    "    if model == NMF:\n",
    "        model = model(num_topics)\n",
    "    elif model == LatentDirichletAllocation:\n",
    "        model = model(n_components=num_topics)\n",
    "        \n",
    "    doc_topic = model.fit_transform(matrix)\n",
    "    topic_word = model.components_\n",
    "    \n",
    "    words = document_term_matrix(df_videos_cleaned_v7, CountVectorizer)[1]\n",
    "    t_model = topic_word.argsort(axis=1)[:, -1:-(num_words+1):-1]\n",
    "    top_topic_words = [[words[i] for i in topic] for topic in t_model]\n",
    "        \n",
    "    return top_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_of_adjectives(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords) \n",
    "    Output: Dataframe with only adjectives in the transcript corpus\n",
    "    '''\n",
    "    def adjectives(cleaned_text):\n",
    "        \n",
    "        preprocessed_text_adj = [(word.lower(), pos) for word, pos in cleaned_text \n",
    "                                    if pos=='ADJ'] \n",
    "        \n",
    "        return preprocessed_text_adj\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(adjectives)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_of_nouns(df):\n",
    "    '''\n",
    "    Input: Cleaned dataframe (after removing custom stopwords) \n",
    "    Output: Dataframe with only nouns in the transcript corpus\n",
    "    '''\n",
    "    def nouns(cleaned_text):\n",
    "        \n",
    "        preprocessed_text_noun = [(word.lower(), pos) for word, pos in cleaned_text \n",
    "                                    if pos=='NOUN'] \n",
    "        \n",
    "        return preprocessed_text_noun\n",
    "    \n",
    "    df['Transcript'] = df['Transcript'].apply(nouns)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v7 = remove_custom_stopwords(df_videos_cleaned_v6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating document-term matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = document_term_matrix(df_videos_cleaned_v7, CountVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = document_term_matrix(df_videos_cleaned_v7, TfidfVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling - Entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['question', 'sort', 'world', 'industry', 'capital', 'tell'],\n",
       " ['dividend', 'equal', 'plus', 'constant', 'model', 'flow'],\n",
       " ['flow', 'billion', 'debt', 'free', 'revenue', 'ebitda'],\n",
       " ['portfolio', 'dividend', 'etf', 'fund', 'yield', 'income'],\n",
       " ['trade', 'fundamental', 'analysis', 'support', 'news', 'level'],\n",
       " ['ratio', 'pe', 'current', 'profit', 'book', 'equity'],\n",
       " ['option', 'leap', 'trade', 'risk', 'month', 'longterm'],\n",
       " ['graham', 'fund', 'buffett', 'book', 'asset', 'analysis']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_cv, NMF, 8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (NMF), TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['question', 'book', 'sort', 'risk', 'buffett', 'tesla'],\n",
       " ['dividend', 'yield', 'portfolio', 'income', 'increase', 'cent'],\n",
       " ['flow', 'billion', 'debt', 'ebitda', 'revenue', 'free'],\n",
       " ['moat', 'advantage', 'mode', 'competitive', 'economic', 'brand'],\n",
       " ['analysis', 'fundamental', 'technical', 'trader', 'ratio', 'chart'],\n",
       " ['ratio', 'model', 'discount', 'formula', 'calculate', 'flow'],\n",
       " ['music', 'foreign', 'applause', 'thank', 'backbone', 'raider'],\n",
       " ['etf', 'fund', 'portfolio', 'index', 'vanguard', 'mutual']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_tfidf, NMF, 8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['portfolio', 'etf', 'account', 'month', 'thousand', 'channel'],\n",
       " ['flow', 'billion', 'revenue', 'debt', 'free', 'ratio'],\n",
       " ['question', 'risk', 'sort', 'capital', 'fund', 'management'],\n",
       " ['book', 'intrinsic', 'valuation', 'graham', 'asset', 'profit'],\n",
       " ['fund', 'learn', 'buffett', 'tell', 'lose', 'question'],\n",
       " ['fundamental', 'analysis', 'trade', 'ratio', 'tesla', 'trading'],\n",
       " ['dividend', 'yield', 'model', 'income', 'flow', 'plus'],\n",
       " ['moat', 'product', 'advantage', 'cost', 'brand', 'competitive']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_cv, LatentDirichletAllocation, 8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling - Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_v7_adj = df_videos_cleaned_v7.copy()\n",
    "df_videos_cleaned_v7_noun = df_videos_cleaned_v7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_cleaned_adj = corpus_of_adjectives(df_videos_cleaned_v7_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_adj = document_term_matrix(df_videos_cleaned_adj, CountVectorizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_adj = document_term_matrix(df_videos_cleaned_adj, TfidfVectorizer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['carpet', 'advent', 'admiralty', 'cheese', 'airspace', 'conver'],\n",
       " ['brokendown', 'buoy', 'airbnbs', 'brandon', 'calibration', 'analytically'],\n",
       " ['arrogant', 'alumnus', 'campus', 'chapman', 'allergy', 'bert'],\n",
       " ['avalanche', 'cofounded', 'arcg', 'brownforman', 'cheese', 'admiralty'],\n",
       " ['chose', 'binding', 'advent', 'arrange', 'adjunct', 'banner'],\n",
       " ['analytically', 'comb', 'brandon', 'adjunct', 'ackman', 'buffet'],\n",
       " ['automat', 'adjunct', 'boundless', 'branch', 'brandon', 'bert'],\n",
       " ['climbs', 'binding', 'arcg', 'allocation', 'boundless', 'adjunct']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_cv_adj, NMF, 8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['carpet', 'advent', 'chose', 'binding', 'admiralty', 'cheese'],\n",
       " ['avalanche', 'cofounded', 'adolescent', 'cheese', 'arcg', 'buoy'],\n",
       " ['alumnus', 'arrogant', 'campus', 'allergy', 'brokendown', 'camaro'],\n",
       " ['allocation', 'arcg', 'coat', 'climbs', 'contraction', 'beneficiaries'],\n",
       " ['automat', 'boundless', 'adjunct', 'climbs', 'bueller', 'branch'],\n",
       " ['bert', 'conver', 'assume', 'analytically', 'commentary', 'bigticket'],\n",
       " ['analytically', 'brandon', 'comb', 'adjunct', 'branch', 'ackman'],\n",
       " ['auditor', 'chose', 'brazil', 'awkward', 'candlestick', 'accurate']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_tfidf_adj, NMF, 8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA), CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chose', 'brokendown', 'cheese', 'avalanche', 'brownforman', 'calculus'],\n",
       " ['brokendown', 'arrogant', 'campus', 'alumnus', 'chapman', 'allergy'],\n",
       " ['admiralty', 'advent', 'airspace', 'carpet', 'conver', 'conversely'],\n",
       " ['automat', 'analytically', 'adjunct', 'brandon', 'bert', 'boundless'],\n",
       " ['auditor', 'binding', 'calibration', 'argentine', 'bumps', 'ck'],\n",
       " ['adjunct', 'becky', 'box', 'child', 'carpet', 'ackman'],\n",
       " ['binding', 'chose', 'awkward', 'advent', 'captain', 'airspace'],\n",
       " ['avalanche', 'allocation', 'arcg', 'cofounded', 'climbs', 'bert']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model(X_cv_adj, LatentDirichletAllocation, 8, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
